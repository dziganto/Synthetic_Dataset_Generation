{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Disclaimer: This notebook uses Python 3.5. Sections may not work if you try this with Python 2.X.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 1.13.0\n",
      "Pandas version 0.20.1\n",
      "Matplotlib version 2.0.2\n",
      "Seaborn version 0.7.1\n"
     ]
    }
   ],
   "source": [
    "items = [(\"Numpy\", np), (\"Pandas\", pd), (\"Matplotlib\", matplotlib), (\"Seaborn\", sns)]\n",
    "for item in items:\n",
    "    print(item[0] + \" version \" + str(item[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.read_hdf('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/data/py35/simulated_cleaned_training_data_py35.h5', 'table')\n",
    "y_train = X_train.pop('hired')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get X_test & y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/X_test_py35.pkl\", 'rb') as picklefile: \n",
    "    X_test = pickle.load(picklefile)\n",
    "    \n",
    "with open(\"/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/y_test_py35.pkl\", 'rb') as picklefile: \n",
    "    y_test = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This section shows the machine learning pipeline used to generate scoring thresholds. A further discussion of how this information is used will be included in the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6514772158237356"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_class_prop = y_train.mean()\n",
    "pos_class_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini',\n",
    "                            splitter='best', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features=None, \n",
    "                            random_state=13, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_split=1e-07, \n",
    "                            class_weight=None, \n",
    "                            presort=False)\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent', \n",
    "                        random_state=99, \n",
    "                        constant=None)\n",
    "\n",
    "gbc = GradientBoostingClassifier(loss='deviance', \n",
    "                                 learning_rate=0.1, \n",
    "                                 n_estimators=50, \n",
    "                                 subsample=1.0, \n",
    "                                 criterion='friedman_mse', \n",
    "                                 min_samples_split=2, \n",
    "                                 min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, \n",
    "                                 max_depth=3, \n",
    "                                 min_impurity_split=1e-07, \n",
    "                                 init=None, \n",
    "                                 random_state=123, \n",
    "                                 max_features=None, \n",
    "                                 verbose=0, \n",
    "                                 max_leaf_nodes=None, \n",
    "                                 warm_start=False, \n",
    "                                 presort='auto')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           weights='uniform', \n",
    "                           algorithm='auto', \n",
    "                           leaf_size=30, \n",
    "                           p=2, \n",
    "                           metric='euclidean', \n",
    "                           metric_params=None, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', \n",
    "                        dual=False, \n",
    "                        tol=0.0001, \n",
    "                        C=1.0, \n",
    "                        fit_intercept=True, \n",
    "                        intercept_scaling=1, \n",
    "                        class_weight=None, \n",
    "                        random_state=10, \n",
    "                        solver='liblinear', \n",
    "                        max_iter=100, \n",
    "                        multi_class='ovr', \n",
    "                        verbose=0, \n",
    "                        warm_start=False, \n",
    "                        n_jobs=-1)\n",
    "\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0, \n",
    "                   fit_prior=True, \n",
    "                   class_prior=None)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, \n",
    "                            criterion='gini', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features='auto', \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_split=1e-07, \n",
    "                            bootstrap=True, \n",
    "                            oob_score=False, \n",
    "                            n_jobs=-1, \n",
    "                            random_state=17, \n",
    "                            verbose=0, \n",
    "                            warm_start=False, \n",
    "                            class_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Parameter Grid For RandomCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_param_grid = dict(max_depth=[None, 6, 8], \n",
    "                     min_samples_leaf=range(1,5),\n",
    "                     class_weight=[None, 'balanced'])\n",
    "\n",
    "gbc_param_grid = dict(loss=['deviance','exponential'], \n",
    "                      max_depth=range(2,5), \n",
    "                      learning_rate=[0.001, 0.01, 0.1])\n",
    "\n",
    "knn_param_grid = dict(n_neighbors=range(1, 15, 2), \n",
    "                      weights=('uniform', 'distance'))\n",
    "\n",
    "lr_param_grid = dict(penalty=['l1', 'l2'], \n",
    "                     C=np.geomspace(0.001, 10, num=5),\n",
    "                     class_weight=[None, 'balanced'])\n",
    "\n",
    "rf_param_grid = dict(n_estimators=[50, 100], \n",
    "                     max_depth=[None, 8, 10],\n",
    "                     class_weight=[None, 'balanced'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup Dictionary For Algo_Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "algo_dict = OrderedDict(\n",
    "    (\n",
    "        (\"dt\",(dt, dt_param_grid)),\n",
    "        (\"dummy\",(dummy)),\n",
    "        (\"gbc\",(gbc, gbc_param_grid)),\n",
    "        (\"knn\",(knn, knn_param_grid)),\n",
    "        (\"lr\",(lr, lr_param_grid)), \n",
    "        (\"nb\",(nb)), \n",
    "        (\"rf\",(rf, rf_param_grid))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo_Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algo_report(algo_dict, cv=5, search_flag=1):\n",
    "    '''\n",
    "    Function that generates in-sample and out-of-sample metrics for numerous machine learning algorithms.\n",
    "    \n",
    "    Input:\n",
    "        algo_dict = dictionary with algorithm name as key and model object & parameter grid as values\n",
    "        cv = number of folds for cross validation\n",
    "        search_flag = {0: use default model paramters; 1: use randomized search; 2: use grid search}\n",
    "    Output:\n",
    "        prints a report showing:\n",
    "            1) in-sample negative log-loss value or accuracy (dependent on score function)\n",
    "            2) out-of-sample negative log-loss value or accuracy (dependent on score function)\n",
    "            3) out-of-sample log loss value\n",
    "            4) confusion matrix\n",
    "    '''\n",
    "    for k, v in algo_dict.items():  \n",
    "        if k == \"nb\" or k == \"dummy\":\n",
    "            model = v.fit(X_train, y_train)\n",
    "        else:\n",
    "            if search_flag:\n",
    "                model = RandomizedSearchCV(v[0], v[1], cv=cv, scoring='neg_log_loss')\n",
    "                model.fit(X_train, y_train)\n",
    "            elif search_flag==2:\n",
    "                model = GridSearchCV(v[0], v[1], cv=cv, scoring='neg_log_loss')\n",
    "                model.fit(X_train, y_train)\n",
    "            else: \n",
    "                model = v[0].fit(X_train, y_train)\n",
    "\n",
    "        print(\"[%s]\" % k)\n",
    "        print(\"In-Sample:     {}\\nOut-of_Sample: {}\\nLog_loss:      {}\".format(\n",
    "            round(model.score(X_train, y_train),3), \n",
    "            round(model.score(X_test, y_test),3), \n",
    "            round(log_loss(y_test, model.predict_proba(X_test)),3)))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "        print(\"\\n-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Results for all three scenarios - using only defaults, using random search, and using grid search - can be found below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dt]\n",
      "In-Sample:     1.0\n",
      "Out-of_Sample: 0.592\n",
      "Log_loss:      14.092\n",
      "\n",
      "Confusion Matrix:\n",
      "[[167 189]\n",
      " [219 425]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[dummy]\n",
      "In-Sample:     0.651\n",
      "Out-of_Sample: 0.644\n",
      "Log_loss:      12.296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 356]\n",
      " [  0 644]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[gbc]\n",
      "In-Sample:     0.71\n",
      "Out-of_Sample: 0.68\n",
      "Log_loss:      0.606\n",
      "\n",
      "Confusion Matrix:\n",
      "[[108 248]\n",
      " [ 72 572]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[knn]\n",
      "In-Sample:     0.742\n",
      "Out-of_Sample: 0.613\n",
      "Log_loss:      2.053\n",
      "\n",
      "Confusion Matrix:\n",
      "[[119 237]\n",
      " [150 494]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[lr]\n",
      "In-Sample:     0.666\n",
      "Out-of_Sample: 0.659\n",
      "Log_loss:      0.62\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 61 295]\n",
      " [ 46 598]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[nb]\n",
      "In-Sample:     0.614\n",
      "Out-of_Sample: 0.614\n",
      "Log_loss:      1.101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[176 180]\n",
      " [206 438]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[rf]\n",
      "In-Sample:     1.0\n",
      "Out-of_Sample: 0.625\n",
      "Log_loss:      0.657\n",
      "\n",
      "Confusion Matrix:\n",
      "[[115 241]\n",
      " [134 510]]\n",
      "\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_report(algo_dict, cv=10, search_flag=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dt]\n",
      "In-Sample:     -0.566\n",
      "Out-of_Sample: -0.881\n",
      "Log_loss:      0.881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[151 205]\n",
      " [143 501]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[dummy]\n",
      "In-Sample:     0.651\n",
      "Out-of_Sample: 0.644\n",
      "Log_loss:      12.296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 356]\n",
      " [  0 644]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[gbc]\n",
      "In-Sample:     -0.59\n",
      "Out-of_Sample: -0.605\n",
      "Log_loss:      0.605\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 92 264]\n",
      " [ 62 582]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[knn]\n",
      "In-Sample:     -0.0\n",
      "Out-of_Sample: -0.697\n",
      "Log_loss:      0.697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 85 271]\n",
      " [100 544]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[lr]\n",
      "In-Sample:     -0.621\n",
      "Out-of_Sample: -0.62\n",
      "Log_loss:      0.62\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 62 294]\n",
      " [ 45 599]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[nb]\n",
      "In-Sample:     0.614\n",
      "Out-of_Sample: 0.614\n",
      "Log_loss:      1.101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[176 180]\n",
      " [206 438]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[rf]\n",
      "In-Sample:     -0.491\n",
      "Out-of_Sample: -0.615\n",
      "Log_loss:      0.615\n",
      "\n",
      "Confusion Matrix:\n",
      "[[102 254]\n",
      " [ 73 571]]\n",
      "\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_report(algo_dict, cv=10, search_flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dt]\n",
      "In-Sample:     -0.566\n",
      "Out-of_Sample: -0.881\n",
      "Log_loss:      0.881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[151 205]\n",
      " [143 501]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[dummy]\n",
      "In-Sample:     0.651\n",
      "Out-of_Sample: 0.644\n",
      "Log_loss:      12.296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 356]\n",
      " [  0 644]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[gbc]\n",
      "In-Sample:     -0.574\n",
      "Out-of_Sample: -0.608\n",
      "Log_loss:      0.608\n",
      "\n",
      "Confusion Matrix:\n",
      "[[105 251]\n",
      " [ 73 571]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[knn]\n",
      "In-Sample:     -0.0\n",
      "Out-of_Sample: -0.697\n",
      "Log_loss:      0.697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 85 271]\n",
      " [100 544]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[lr]\n",
      "In-Sample:     -0.621\n",
      "Out-of_Sample: -0.62\n",
      "Log_loss:      0.62\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 62 294]\n",
      " [ 45 599]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[nb]\n",
      "In-Sample:     0.614\n",
      "Out-of_Sample: 0.614\n",
      "Log_loss:      1.101\n",
      "\n",
      "Confusion Matrix:\n",
      "[[176 180]\n",
      " [206 438]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[rf]\n",
      "In-Sample:     -0.491\n",
      "Out-of_Sample: -0.615\n",
      "Log_loss:      0.615\n",
      "\n",
      "Confusion Matrix:\n",
      "[[102 254]\n",
      " [ 73 571]]\n",
      "\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_report(algo_dict, cv=10, search_flag=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models For Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69509316338861982"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs Improvement Model\n",
    "knn_randcv = RandomizedSearchCV(knn, knn_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "knn_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, knn_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61533035544578418"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Satisfactory Model\n",
    "rf_randcv = RandomizedSearchCV(rf, rf_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "rf_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, rf_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.60513366403065194"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proficient Model\n",
    "gbc_randcv = RandomizedSearchCV(gbc, gbc_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "gbc_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, gbc_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Models & Test Set Data For Auto-Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = '/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/'\n",
    "\n",
    "# Save KNN model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/knn_needs_improvement_py35.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(knn_randcv, picklefile)\n",
    "    \n",
    "# Save Random Forest model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/rf_satisfactory_py35.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(rf_randcv, picklefile)\n",
    "    \n",
    "# Save Gradient Boosted Classifier model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/gbc_proficient_py35.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(gbc_randcv, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
