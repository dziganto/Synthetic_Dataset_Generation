{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data & Models For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/\"\n",
    "\n",
    "with open(path + \"X_test_py35.pkl\", 'rb') as picklefile: \n",
    "    X_test = pickle.load(picklefile)\n",
    "    \n",
    "with open(path + \"y_test_py35.pkl\", 'rb') as picklefile: \n",
    "    y_test = pickle.load(picklefile)\n",
    "\n",
    "with open(path + \"knn_needs_improvement_py35.pkl\", 'rb') as picklefile: \n",
    "    knn_needs_improvement = pickle.load(picklefile)\n",
    "    \n",
    "with open(path + \"rf_satisfactory_py35.pkl\", 'rb') as picklefile: \n",
    "    rf_satisfactory = pickle.load(picklefile)\n",
    "\n",
    "with open(path + \"gbc_proficient_py35.pkl\", 'rb') as picklefile: \n",
    "    gbc_proficient = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75208153985645276"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, knn_needs_improvement.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.53246821862725746"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, rf_satisfactory.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51749469915046675"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, gbc_proficient.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pickle_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dict = {'knn':knn_needs_improvement, 'rf_':rf_satisfactory, 'gbc':gbc_proficient}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_score(pickle_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        pickle_dict: dictionary where key is username | ID and value is model\n",
    "    Output:\n",
    "        username : [log loss value, classification] \n",
    "    '''\n",
    "    for k, v in pickle_dict.items():\n",
    "        score = log_loss(y_test, v.predict_proba(X_test))\n",
    "        if score < 0.530:\n",
    "            pickle_dict[k] = [score, \"Proficient\"]\n",
    "        elif score <= 0.540:\n",
    "            pickle_dict[k] = [score, \"Satisfactory\"]\n",
    "        else:\n",
    "            pickle_dict[k] = [score, \"Needs Improvement\"]\n",
    "            \n",
    "    return pickle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc': [0.51749469915046675, 'Proficient'],\n",
       " 'knn': [0.75208153985645276, 'Needs Improvement'],\n",
       " 'rf_': [0.53246821862725746, 'Satisfactory']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = auto_score(pickle_dict)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc 0.51749469915 Proficient\n",
      "rf_ 0.532468218627 Satisfactory\n",
      "knn 0.752081539856 Needs Improvement\n"
     ]
    }
   ],
   "source": [
    "for k,v in output.items():\n",
    "    print(k, output[k][0], output[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rationale For Scoring\n",
    "\n",
    "Default settings for Decision Trees, KNN, Logistic Regression, Multinomial Naive Bayes, and Random Forest yield log loss values on the test set of:\n",
    "\n",
    "|Algo|Log Loss|Type|\n",
    "|---|---|---|\n",
    "|DT|12.676|Std|\n",
    "|GBC|0.517|Adv|\n",
    "|KNN|2.064|Std|\n",
    "|LR|0.543|Std|\n",
    "|NB|0.963|Std|\n",
    "|RF|0.566|Std|\n",
    "\n",
    "\n",
    "**Std=standard algorithm**; \n",
    "**Adv=advanced algorithm**\n",
    "\n",
    "Tuned versions yield the following log loss values:\n",
    "\n",
    "|Algo|Log Loss|Type|\n",
    "|---|---|---|\n",
    "|DT|0.797|Std|\n",
    "|GBC|0.517|Adv|\n",
    "|KNN|0.752|Std|\n",
    "|LR|0.543|Std|\n",
    "|NB|0.963|Std|\n",
    "|RF|0.532|Std|\n",
    "\n",
    "The goal here is to determine each invidual's skill level in achieving performant modeling results. As such, it made sense to set the threshold for *satisfactory* just below the lowest log loss value yielded by the default model settings, which is logistic regression at 0.543. Therefore, the threshold is set at 0.540.\n",
    "\n",
    "Furthermore, in an attempt to separate the high-achieving students, a category called *proficient* is included. In order to achieve this status, a student must use modeling techniques either not covered or covered only in very little detail to achieve the required log loss value. Therefore, the threshold (0.530) was set just below the log loss value of the lowest log loss value for the 'stock' algorithms, which is tuned random forest (0.532). For instance, a tuned gradient boosted classifier can achieve a log loss of 0.517.\n",
    "\n",
    "**NOTES**   \n",
    "1. There will be some variablity with inherently non-deterministic algorithms like random forest. The techniques will be the same but the results may vary slightly due to how the model was seeded.  \n",
    "2.  This is a first attempt. Score thresholds can be adjusted as we collect data in our pilot program. In other words, this is a WIP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
