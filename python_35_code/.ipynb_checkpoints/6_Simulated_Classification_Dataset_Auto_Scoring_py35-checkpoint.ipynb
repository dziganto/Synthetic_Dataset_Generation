{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data & Models For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path = \"/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py35/\"\n",
    "\n",
    "with open(path + \"X_test_py35.pkl\", 'rb') as picklefile: \n",
    "    X_test = pickle.load(picklefile)\n",
    "    \n",
    "with open(path + \"y_test_py35.pkl\", 'rb') as picklefile: \n",
    "    y_test = pickle.load(picklefile)\n",
    "\n",
    "with open(path + \"knn_needs_improvement_py35.pkl\", 'rb') as picklefile: \n",
    "    knn_needs_improvement = pickle.load(picklefile)\n",
    "    \n",
    "with open(path + \"rf_satisfactory_py35.pkl\", 'rb') as picklefile: \n",
    "    rf_satisfactory = pickle.load(picklefile)\n",
    "\n",
    "with open(path + \"gbc_proficient_py35.pkl\", 'rb') as picklefile: \n",
    "    gbc_proficient = pickle.load(picklefile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Log Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.73921970647492385"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, knn_needs_improvement.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59110226449157899"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, rf_satisfactory.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.58273276192776557"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, gbc_proficient.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Pickle_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickle_dict = {'knn':knn_needs_improvement, 'rf_':rf_satisfactory, 'gbc':gbc_proficient}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto_Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_score(pickle_dict):\n",
    "    '''\n",
    "    Input:\n",
    "        pickle_dict: dictionary where key is username | ID and value is model\n",
    "    Output:\n",
    "        username : [log loss value, classification] \n",
    "    '''\n",
    "    for k, v in pickle_dict.items():\n",
    "        score = log_loss(y_test, v.predict_proba(X_test))\n",
    "        if score < 0.59:\n",
    "            pickle_dict[k] = [score, \"Proficient\"]\n",
    "        elif score <= 0.62:\n",
    "            pickle_dict[k] = [score, \"Satisfactory\"]\n",
    "        else:\n",
    "            pickle_dict[k] = [score, \"Needs Improvement\"]\n",
    "            \n",
    "    return pickle_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gbc': [0.58273276192776557, 'Proficient'],\n",
       " 'knn': [0.73921970647492385, 'Needs Improvement'],\n",
       " 'rf_': [0.59110226449157899, 'Satisfactory']}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = auto_score(pickle_dict)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gbc 0.582732761928 Proficient\n",
      "rf_ 0.591102264492 Satisfactory\n",
      "knn 0.739219706475 Needs Improvement\n"
     ]
    }
   ],
   "source": [
    "for k,v in output.items():\n",
    "    print(k, output[k][0], output[k][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rationale For Scoring\n",
    "\n",
    "Default settings for Decision Trees, KNN, Logistic Regression, Multinomial Naive Bayes, and Random Forest yield log loss values on the test set of:\n",
    "\n",
    "|Algo|Log Loss|Type|\n",
    "|---|---|---|\n",
    "|DT|13.228|Std|\n",
    "|GBC|0.582|Adv|\n",
    "|KNN|2.321|Std|\n",
    "|LR|0.589|Std|\n",
    "|NB|0.922|Std|\n",
    "|RF|0.622|Std|\n",
    "\n",
    "\n",
    "**Std=standard algorithm**; \n",
    "**Adv=advanced algorithm**\n",
    "\n",
    "Tuned versions yield the following log loss values:\n",
    "\n",
    "|Algo|Log Loss|Type|\n",
    "|---|---|---|\n",
    "|DT|0.976|Std|\n",
    "|GBC|0.583|Adv|\n",
    "|KNN|0.739|Std|\n",
    "|LR|0.590|Std|\n",
    "|NB|0.922|Std|\n",
    "|RF|0.591|Std|\n",
    "\n",
    "The goal here is to determine each invidual's skill level in achieving performant modeling results. As such, it made sense to set the threshold for *satisfactory* at a level below all but the lowest of log loss values yielded by default model settings. The threshold is set at 0.62.\n",
    "\n",
    "Furthermore, in an attempt to separate the high-achieving students, a category called *proficient* is included. In order to achieve this status, a student must use modeling techniques either not covered or covered only in very little detail to achieve the required log loss value. Therefore, the threshold (0.590) was set just below the log loss value of a tuned random forest (0.591). For instance, a tuned gradient boosted classifier can achieve a log loss of 0.583.\n",
    "\n",
    "**NOTES**\n",
    "1. You may notice that running a stock logistic regression model will place a student in the *satisfactory* bucket. This is not ideal. The original goal was to set the satisfactory threshold just below the lowest log loss value achieved by default models. The dataset will require tweaking to get enough variance for this to make sense, so this is something that should be addressed in the next iteration.   \n",
    "2. There will be some variablity with inherently non-deterministic algorithms like random forest. The techniques will be the same but the results may vary slightly due to how the model was seeded.  \n",
    "3.  This is a first attempt. Score thresholds can be adjusted as we collect data in our pilot program. In other words, this is a WIP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
