{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from copy import deepcopy\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 2.7.13 :: Anaconda 4.4.0 (x86_64)\r\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy version 1.13.0\n",
      "Pandas version 0.20.1\n",
      "Matplotlib version 2.0.2\n",
      "Seaborn version 0.7.1\n"
     ]
    }
   ],
   "source": [
    "items = [(\"Numpy\", np), (\"Pandas\", pd), (\"Matplotlib\", matplotlib), (\"Seaborn\", sns)]\n",
    "for item in items:\n",
    "    print(item[0] + \" version \" + str(item[1].__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Read from disk\n",
    "data = pd.read_hdf('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/data/py27/simulated_cleaned_data_py27.h5', 'table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "X = deepcopy(data)\n",
    "y = X.pop('hired')\n",
    "\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Class Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.690863579474343"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_class_prop = y_train.mean()\n",
    "pos_class_prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.dummy import DummyClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(criterion='gini',\n",
    "                            splitter='best', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features=None, \n",
    "                            random_state=13, \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_split=1e-07, \n",
    "                            class_weight={0:(1-pos_class_prop), 1:pos_class_prop}, \n",
    "                            presort=False)\n",
    "\n",
    "dummy = DummyClassifier(strategy='most_frequent', \n",
    "                        random_state=99, \n",
    "                        constant=None)\n",
    "\n",
    "gbc = GradientBoostingClassifier(loss='deviance', \n",
    "                                 learning_rate=0.1, \n",
    "                                 n_estimators=50, \n",
    "                                 subsample=1.0, \n",
    "                                 criterion='friedman_mse', \n",
    "                                 min_samples_split=2, \n",
    "                                 min_samples_leaf=1, \n",
    "                                 min_weight_fraction_leaf=0.0, \n",
    "                                 max_depth=3, \n",
    "                                 min_impurity_split=1e-07, \n",
    "                                 init=None, \n",
    "                                 random_state=123, \n",
    "                                 max_features=None, \n",
    "                                 verbose=0, \n",
    "                                 max_leaf_nodes=None, \n",
    "                                 warm_start=False, \n",
    "                                 presort='auto')\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5, \n",
    "                           weights='uniform', \n",
    "                           algorithm='auto', \n",
    "                           leaf_size=30, \n",
    "                           p=2, \n",
    "                           metric='euclidean', \n",
    "                           metric_params=None, \n",
    "                           n_jobs=-1)\n",
    "\n",
    "lr = LogisticRegression(penalty='l2', \n",
    "                        dual=False, \n",
    "                        tol=0.0001, \n",
    "                        C=1.0, \n",
    "                        fit_intercept=True, \n",
    "                        intercept_scaling=1, \n",
    "                        class_weight={0:(1-pos_class_prop), 1:pos_class_prop}, \n",
    "                        random_state=10, \n",
    "                        solver='liblinear', \n",
    "                        max_iter=100, \n",
    "                        multi_class='ovr', \n",
    "                        verbose=0, \n",
    "                        warm_start=False, \n",
    "                        n_jobs=-1)\n",
    "\n",
    "\n",
    "nb = MultinomialNB(alpha=1.0, \n",
    "                   fit_prior=True, \n",
    "                   class_prior=None)\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=50, \n",
    "                            criterion='gini', \n",
    "                            max_depth=None, \n",
    "                            min_samples_split=2, \n",
    "                            min_samples_leaf=1, \n",
    "                            min_weight_fraction_leaf=0.0, \n",
    "                            max_features='auto', \n",
    "                            max_leaf_nodes=None, \n",
    "                            min_impurity_split=1e-07, \n",
    "                            bootstrap=True, \n",
    "                            oob_score=False, \n",
    "                            n_jobs=-1, \n",
    "                            random_state=17, \n",
    "                            verbose=0, \n",
    "                            warm_start=False, \n",
    "                            class_weight={0:(1-pos_class_prop), 1:pos_class_prop})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Parameter Grid For RandomCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt_param_grid = dict(criterion=['gini','entropy'], max_depth=range(2,7), min_samples_leaf=range(1,5))\n",
    "gbc_param_grid = dict(loss=['deviance','exponential'], max_depth=range(2,5), learning_rate=[0.001, 0.01, 0.1])\n",
    "knn_param_grid = dict(n_neighbors=range(1, 15, 2), weights=('uniform', 'distance'))\n",
    "lr_param_grid = dict(penalty=['l1', 'l2'], C=np.geomspace(0.001, 10, num=5))\n",
    "rf_param_grid = dict(n_estimators=[50,200,500,1000], max_depth=range(6,11,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Setup Dictionary For Algo_Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "algo_dict = OrderedDict(\n",
    "    (\n",
    "        (\"dt\",(dt, dt_param_grid)),\n",
    "        (\"dummy\",(dummy)),\n",
    "        (\"gbc\",(gbc, gbc_param_grid)),\n",
    "        (\"knn\",(knn, knn_param_grid)),\n",
    "        (\"lr\",(lr, lr_param_grid)), \n",
    "        (\"nb\",(nb)), \n",
    "        (\"rf\",(rf, rf_param_grid))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo_Report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def algo_report(algo_dict, cv=5, random_search_flag=1):\n",
    "    '''\n",
    "    Function that generates in-sample and out-of-sample metrics for numerous machine learning algorithms.\n",
    "    \n",
    "    Input:\n",
    "        algo_dict = dictionary with algorithm name as key and model object & parameter grid as values\n",
    "        cv = number of folds for cross validation\n",
    "        random_search_flag = {0: use default model paramters; 1: use randomized search}\n",
    "    Output:\n",
    "        prints a report showing:\n",
    "            1) in-sample negative log-loss value or accuracy (dependent on score function)\n",
    "            2) out-of-sample negative log-loss value or accuracy (dependent on score function)\n",
    "            3) out-of-sample log loss value\n",
    "            4) confusion matrix\n",
    "    '''\n",
    "    for k, v in algo_dict.iteritems():  \n",
    "        if k == \"nb\" or k == \"dummy\":\n",
    "            model = v.fit(X_train, y_train)\n",
    "        else:\n",
    "            if random_search_flag:\n",
    "                model = RandomizedSearchCV(v[0], v[1], cv=cv, scoring='neg_log_loss')\n",
    "                model.fit(X_train, y_train)\n",
    "            else: \n",
    "                model = v[0].fit(X_train, y_train)\n",
    "\n",
    "        print(\"[%s]\" % k)\n",
    "        print(\"In-Sample:     {}\\nOut-of_Sample: {}\\nLog_loss:      {}\".format(\n",
    "            round(model.score(X_train, y_train),3), \n",
    "            round(model.score(X_test, y_test),3), \n",
    "            round(log_loss(y_test, model.predict_proba(X_test)),3)))\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, model.predict(X_test)))\n",
    "        print(\"\\n-----------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dt]\n",
      "In-Sample:     1.0\n",
      "Out-of_Sample: 0.616\n",
      "Log_loss:      13.276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[143 169]\n",
      " [215 472]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[dummy]\n",
      "In-Sample:     0.691\n",
      "Out-of_Sample: 0.688\n",
      "Log_loss:      10.787\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 312]\n",
      " [  0 687]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[gbc]\n",
      "In-Sample:     0.746\n",
      "Out-of_Sample: 0.734\n",
      "Log_loss:      0.543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[105 207]\n",
      " [ 59 628]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[knn]\n",
      "In-Sample:     0.764\n",
      "Out-of_Sample: 0.661\n",
      "Log_loss:      2.348\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 82 230]\n",
      " [109 578]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[lr]\n",
      "In-Sample:     0.693\n",
      "Out-of_Sample: 0.687\n",
      "Log_loss:      0.64\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  1 311]\n",
      " [  2 685]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[nb]\n",
      "In-Sample:     0.647\n",
      "Out-of_Sample: 0.658\n",
      "Log_loss:      1.032\n",
      "\n",
      "Confusion Matrix:\n",
      "[[145 167]\n",
      " [175 512]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[rf]\n",
      "In-Sample:     1.0\n",
      "Out-of_Sample: 0.712\n",
      "Log_loss:      0.608\n",
      "\n",
      "Confusion Matrix:\n",
      "[[117 195]\n",
      " [ 93 594]]\n",
      "\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_report(algo_dict, cv=10, random_search_flag=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dt]\n",
      "In-Sample:     -0.605\n",
      "Out-of_Sample: -0.613\n",
      "Log_loss:      0.613\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  5 307]\n",
      " [  4 683]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[dummy]\n",
      "In-Sample:     0.691\n",
      "Out-of_Sample: 0.688\n",
      "Log_loss:      10.787\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  0 312]\n",
      " [  0 687]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[gbc]\n",
      "In-Sample:     -0.542\n",
      "Out-of_Sample: -0.542\n",
      "Log_loss:      0.542\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 84 228]\n",
      " [ 37 650]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[knn]\n",
      "In-Sample:     -0.541\n",
      "Out-of_Sample: -0.772\n",
      "Log_loss:      0.772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 68 244]\n",
      " [ 62 625]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[lr]\n",
      "In-Sample:     -0.641\n",
      "Out-of_Sample: -0.639\n",
      "Log_loss:      0.639\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  1 311]\n",
      " [  2 685]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[nb]\n",
      "In-Sample:     0.647\n",
      "Out-of_Sample: 0.658\n",
      "Log_loss:      1.032\n",
      "\n",
      "Confusion Matrix:\n",
      "[[145 167]\n",
      " [175 512]]\n",
      "\n",
      "-----------------\n",
      "\n",
      "[rf]\n",
      "In-Sample:     -0.361\n",
      "Out-of_Sample: -0.575\n",
      "Log_loss:      0.575\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 51 261]\n",
      " [ 28 659]]\n",
      "\n",
      "-----------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "algo_report(algo_dict, cv=10, random_search_flag=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Models For Pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77172596402024607"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Needs Improvement Model\n",
    "knn_randcv = RandomizedSearchCV(knn, knn_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "knn_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, knn_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57484754370751456"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Satisfactory Model\n",
    "rf_randcv = RandomizedSearchCV(rf, rf_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "rf_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, rf_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54224558841354176"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proficient Model\n",
    "gbc_randcv = RandomizedSearchCV(gbc, gbc_param_grid, cv=10, scoring='neg_log_loss', random_state=42)\n",
    "gbc_randcv.fit(X_train, y_train)\n",
    "log_loss(y_test, gbc_randcv.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Models & Test Set Data For Auto-Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save X_test\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py27/X_test_py27.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(X_test, picklefile)\n",
    "    \n",
    "# Save y_test\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py27/y_test_py27.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(y_test, picklefile)\n",
    "\n",
    "# Save KNN model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py27/knn_needs_improvement_py27.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(knn_randcv, picklefile)\n",
    "    \n",
    "# Save Random Forest model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py27/rf_satisfactory_py27.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(rf_randcv, picklefile)\n",
    "    \n",
    "# Save Gradient Boosted Classifier model\n",
    "with open('/Users/davidziganto/Repositories/Synthetic_Dataset_Generation/pickle_files/py27/gbc_proficient_py27.pkl', 'wb') as picklefile:\n",
    "    pickle.dump(gbc_randcv, picklefile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
